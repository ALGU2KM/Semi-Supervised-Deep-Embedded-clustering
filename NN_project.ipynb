{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "import math\n",
    "from torch import nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "import matplotlib\n",
    "matplotlib.use('agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "# %matplotlib inline  \n",
    "from torch.utils.data.sampler import Sampler, SubsetRandomSampler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "from sklearn.metrics.cluster import adjusted_rand_score, adjusted_mutual_info_score, normalized_mutual_info_score\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from sklearn.metrics import roc_auc_score as AUC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.stats import mode\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.special import erf\n",
    "from sklearn.metrics import average_precision_score as PR_AUC\n",
    "import copy\n",
    "from scipy.io import arff\n",
    "import os\n",
    "import Utils.utils as utils\n",
    "from sklearn.manifold import TSNE\n",
    "# import hdbscan\n",
    "from Utils.utils import acc, BalancedBatchSampler\n",
    "from model import VAE as CVAE\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn import preprocessing\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import itertools\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset1(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "#         data = pd.read_csv('../Data/KDDCup99/KDDCup99_withoutdupl_norm_1ofn.csv', header=None)\n",
    "#         data = pd.read_csv('../Data/Synthetic/dim_8_cluster_1.csv/1_8_10000_0.5/dim.csv', header=None)\n",
    "        features = pd.read_csv(f'../Data/Dataset/{dataset}/dim.csv', header=None)\n",
    "        labels = pd.read_csv(f'../Data/Dataset/{dataset}/label.csv', header=None)\n",
    "        self.features = features.values\n",
    "        self.train_labels = np.squeeze(labels.values)\n",
    "        self.mean = np.mean(self.features, axis=0)\n",
    "        self.std = np.std(self.features, axis=0)\n",
    "# #         self.min = np.amin(self.features, axis=0)\n",
    "# #         self.max = np.amax(self.features, axis=0)\n",
    "#         self.features = (self.features - self.mean)/self.std\n",
    "#         self.features = np.nan_to_num(self.features)\n",
    "        self.data_len = len(self.features)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        feature = self.features[index]\n",
    "\n",
    "        feature_as_tensor = torch.from_numpy(feature).float()\n",
    "\n",
    "        # Get label(class) of the image based on the cropped pandas column\n",
    "        label = self.train_labels[index]\n",
    "\n",
    "        return (feature_as_tensor, label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_len\n",
    "    \n",
    "class CustomDataset2(Dataset):\n",
    "    def __init__(self):\n",
    "        cats = ['alt.atheism',\n",
    "                 'comp.graphics',\n",
    "                 'comp.os.ms-windows.misc',\n",
    "                 'comp.sys.ibm.pc.hardware',\n",
    "                 'comp.sys.mac.hardware',\n",
    "                 'comp.windows.x',\n",
    "                 'misc.forsale',\n",
    "                 'rec.autos',\n",
    "                 'rec.motorcycles',\n",
    "                 'rec.sport.baseball']\n",
    "        newsgroups_train = fetch_20newsgroups(subset='train', categories=cats,\n",
    "                                      remove=('headers', 'footers', 'quotes'))\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        self.features = vectorizer.fit_transform(newsgroups_train.data).toarray()\n",
    "        self.labels =  newsgroups_train.target\n",
    "        self.data_len = len(self.features)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        feature = self.features[index]\n",
    "\n",
    "        feature_as_tensor = torch.from_numpy(feature).float()\n",
    "\n",
    "        # Get label(class) of the image based on the cropped pandas column\n",
    "        label = self.labels[index]\n",
    "\n",
    "        return (feature_as_tensor, label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_len\n",
    "\n",
    "class CustomDataset3(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        \n",
    "        self.train = True\n",
    "        self.X = torch.from_numpy(X).float()\n",
    "        self.train_labels = torch.from_numpy(y)\n",
    "        self.data_len = len(y)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        x = self.X[index]\n",
    "\n",
    "        # Get label(class) of the image based on the cropped pandas column\n",
    "        label = self.train_labels[index]\n",
    "\n",
    "        return (x, label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_len\n",
    "\n",
    "class CustomDataset4(Dataset):\n",
    "    def __init__(self, X):\n",
    "        \n",
    "        self.train = True\n",
    "        self.X = torch.LongTensor(X)\n",
    "        self.data_len = len(X)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        x = self.X[index]\n",
    "        return x\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_len\n",
    "    \n",
    "class MySampler(Sampler):\n",
    "    def __init__(self, mask):\n",
    "        self.mask = torch.from_numpy(mask)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return (self.indices[i] for i in torch.nonzero(self.mask))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_bn_eval(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('BatchNorm') != -1:\n",
    "        m.eval()\n",
    "\n",
    "def set_bn_train(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('BatchNorm') != -1:\n",
    "        m.train()\n",
    "\n",
    "def check_bn_eval(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('BatchNorm') != -1:\n",
    "        if(m.training):\n",
    "            print('problem')\n",
    "        else:\n",
    "            print('converted BN **')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE(nn.Module):\n",
    "    def __init__(self, ndims=784, nlatent=10):\n",
    "        super(AE, self).__init__()\n",
    "        self.ndims = ndims\n",
    "        self.nlatent = nlatent\n",
    "        self.fc1 = self.custom_linear(ndims, 500)\n",
    "        self.fc2 = self.custom_linear(500, 500)\n",
    "        self.fc3 = self.custom_linear(500, 2000)\n",
    "        self.fc4 = nn.Linear(2000, nlatent)\n",
    "        self.fc5 = self.custom_linear(nlatent, 2000)\n",
    "        self.fc6 = self.custom_linear(2000, 500)\n",
    "        self.fc7 = self.custom_linear(500, 500)\n",
    "        self.fc8 = self.custom_linear(500, ndims)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = self.fc1(x)\n",
    "        h2 = self.fc2(h1)\n",
    "        h3 = self.fc3(h2)\n",
    "        return self.fc4(h3)\n",
    "\n",
    "    def decode(self, z):\n",
    "        h1 = self.fc5(z)\n",
    "        h2 = self.fc6(h1)\n",
    "        h3 = self.fc7(h2)\n",
    "        return self.sigmoid(self.fc8(h3))\n",
    "\n",
    "    def forward(self, x, vae=0):\n",
    "        x = x.view(-1, self.ndims)\n",
    "        z = self.encode(x)\n",
    "        return z, self.decode(z), None, None\n",
    "\n",
    "    def custom_linear(self, in_size, out_size):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(in_size, out_size),\n",
    "            nn.BatchNorm1d(out_size),\n",
    "            nn.ReLU(),\n",
    "        ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecTvae(nn.Module):\n",
    "    def __init__(self, dataset='MNIST_NN', ndim=32, nchannels=1, n_clusters=10, z_dim=10, kernel_num=128, \n",
    "                 lr=0.001, weight_decay=1e-03, batch_size=120, n_epochs=3, alpha=1., gamma=0.1, vae=0):\n",
    "        super(self.__class__, self).__init__()\n",
    "        self.z_dim = z_dim\n",
    "        self.vae = vae\n",
    "        self.dataset = dataset\n",
    "        self.ndim = ndim\n",
    "        self.nchannels = nchannels\n",
    "        self.kernel_num = kernel_num\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.batch_size = batch_size\n",
    "        self.n_epochs = n_epochs\n",
    "#         self.model = CVAE(dataset, ndim, nchannels, kernel_num, z_dim)\n",
    "        self.model = AE(ndim*ndim, z_dim) \n",
    "        self.out_linear = nn.Linear(z_dim, n_clusters)\n",
    "        self.cross_entropy = nn.CrossEntropyLoss()\n",
    "#         self.model = VAE(ndims=1024)\n",
    "        self.oneD = False\n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "        self.results_dir = f'../Results/dectvae/{dataset}/'\n",
    "        self.models_dir = f'Models/dectvae/{dataset}/'\n",
    "        self.MSELoss = nn.MSELoss()\n",
    "        self.triplet_loss = nn.TripletMarginLoss()\n",
    "        if(not os.path.exists(self.results_dir)):\n",
    "            os.makedirs(self.results_dir)\n",
    "        if(not os.path.exists(self.models_dir)):\n",
    "            os.makedirs(self.models_dir)\n",
    "        if(self.use_cuda):\n",
    "#             self.model.cuda()\n",
    "            self.cross_entropy.cuda()\n",
    "        utils.initialize_weights(self.model)\n",
    "        self.n_clusters = n_clusters\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.mu = Parameter(torch.Tensor(n_clusters, z_dim))\n",
    "        self.reorder = None\n",
    "\n",
    "    def Variational(self, vae):\n",
    "        self.vae = vae\n",
    "        \n",
    "    def save_model(self, path):\n",
    "        torch.save(self.state_dict(), f'{self.models_dir}/{path}')\n",
    "\n",
    "    def load_model(self, path):\n",
    "        pretrained_dict = torch.load(f'{self.models_dir}/{path}', map_location=lambda storage, loc: storage)\n",
    "        model_dict = self.state_dict()\n",
    "        pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "        model_dict.update(pretrained_dict) \n",
    "        self.load_state_dict(model_dict)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z, mux, mean, logvar = self.model(x, self.vae)\n",
    "        if(self.vae):\n",
    "            return z, mux, mean, logvar\n",
    "        # compute q -> NxK\n",
    "        q = self.soft_assign(z)\n",
    "        return z, q, mux\n",
    "\n",
    "    def soft_assign(self, z):\n",
    "        q = 1.0 / (1.0 + torch.sum((z.unsqueeze(1) - self.mu)**2, dim=2) / self.alpha)\n",
    "        q = q**(self.alpha+1.0)/2.0\n",
    "        q = q / torch.sum(q, dim=1, keepdim=True)\n",
    "        return q\n",
    "\n",
    "    def encodeBatch(self, X):\n",
    "        batch_size = self.batch_size\n",
    "        use_cuda = torch.cuda.is_available()\n",
    "        if use_cuda:\n",
    "            self.cuda()\n",
    "        \n",
    "        encoded = []\n",
    "#         self.eval()\n",
    "        num = X.shape[0]\n",
    "        num_batch = int(math.ceil(1.0*X.shape[0]/batch_size))\n",
    "        for batch_idx in range(num_batch):\n",
    "            xbatch = X[batch_idx*batch_size : min((batch_idx+1)*batch_size, num)]\n",
    "            inputs = Variable(xbatch)\n",
    "            if(use_cuda):\n",
    "                inputs = inputs.cuda()\n",
    "#             print(inputs.size())\n",
    "            z,_, _ = self.forward(inputs)\n",
    "            encoded.append(z.data)\n",
    "\n",
    "        encoded = torch.cat(encoded, dim=0)\n",
    "#         print(encoded.size())\n",
    "        return encoded\n",
    "\n",
    "\n",
    "    def loss_function(self, x, xrecon, p, q):\n",
    "        def kld(target, pred):\n",
    "            return torch.mean(torch.sum(target*torch.log(target/(pred+1e-6)), dim=1))\n",
    "        \n",
    "        kldloss = kld(p, q)\n",
    "        recon_loss = nn.BCELoss()(xrecon, x.view(-1, self.ndim*self.ndim))#torch.mean((xrecon-x)**2)\n",
    "        loss = recon_loss + self.gamma*kldloss\n",
    "        '''\n",
    "        experiment: returning kldloss\n",
    "        '''\n",
    "        return loss\n",
    "\n",
    "    def target_distribution(self, q):\n",
    "        p = q**2 / torch.sum(q, dim=0)\n",
    "        p = p / torch.sum(p, dim=1, keepdim=True)\n",
    "        return p\n",
    "\n",
    "    def eval_clustering(self, validloader):\n",
    "        if(self.use_cuda):\n",
    "            self.cuda()\n",
    "        self.eval()\n",
    "        z_agg = []\n",
    "        y = []\n",
    "        kmeans = KMeans(self.n_clusters, n_init=20)\n",
    "        for batch_idx, (inputs, labels) in enumerate(validloader):\n",
    "            inputs = Variable(inputs)\n",
    "            if(self.use_cuda):\n",
    "                inputs = inputs.cuda()\n",
    "            z, _, mux = self.forward(inputs)\n",
    "            z_agg.append(z)\n",
    "            y.extend(labels.data.cpu().numpy())\n",
    "\n",
    "        y = np.array(y)\n",
    "        latent = torch.cat(z_agg, dim=0)\n",
    "        y_pred = kmeans.fit_predict(latent.data.cpu().numpy())\n",
    "        if y is not None:\n",
    "            print(\"***Kmeans Validation acc: %.5f, nmi: %.5f\" % (acc(y, y_pred), normalized_mutual_info_score(y, y_pred)))\n",
    "\n",
    "\n",
    "        q = self.soft_assign(latent)\n",
    "        # evalute the clustering performance\n",
    "        y_pred = torch.argmax(q, dim=1).data.cpu().numpy()\n",
    "        if y is not None:\n",
    "            print(\"***Validation acc: %.5f, nmi: %.5f\" % (acc(y, y_pred), normalized_mutual_info_score(y, y_pred)))\n",
    "    \n",
    "    def pretrain(self, trainloader, validloader):\n",
    "        if(self.use_cuda):\n",
    "            self.cuda()\n",
    "        optimizer = optim.Adam(filter(lambda p: p.requires_grad, self.parameters()), lr=self.lr)\n",
    "\n",
    "        for epoch in range(self.n_epochs):\n",
    "            self.train()\n",
    "            assert self.model.training == True\n",
    "            tloss = 0\n",
    "            for batch_idx, (inputs, labels) in enumerate(trainloader):\n",
    "                inputs = Variable(inputs)\n",
    "                if(self.use_cuda):\n",
    "                    inputs = inputs.cuda()\n",
    "                z, _, mux = self.forward(inputs)\n",
    "                loss = nn.BCELoss()(mux, inputs.view(-1, self.ndim*self.ndim))\n",
    "                tloss += loss.item()\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            print(epoch, \":\", tloss/len(trainloader))\n",
    "            \n",
    "#             if(epoch%20 == 0):\n",
    "#                 self.base_clustering(validloader)\n",
    "        \n",
    "   \n",
    "    def train_classifier(self, trainloader):\n",
    "        if(self.use_cuda):\n",
    "            self.cuda()\n",
    "        self.train()\n",
    "        optimizer = optim.Adam(filter(lambda p: p.requires_grad, self.parameters()), lr=self.lr)\n",
    "        loss_T = 0\n",
    "        for _ in range(20):\n",
    "            Y = []\n",
    "            Y_pred = []\n",
    "            for inputs, labels in trainloader:\n",
    "                if(self.use_cuda):\n",
    "                    inputs = inputs.cuda()\n",
    "                    labels = labels.cuda()\n",
    "\n",
    "                inputs = Variable(inputs)\n",
    "\n",
    "                z, q, _ = self.forward(inputs)\n",
    "                logits = self.out_linear(z)\n",
    "                loss = self.cross_entropy(logits, labels)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                Y.append(labels.cpu())\n",
    "                y_pred = torch.argmax(logits, 1)\n",
    "                Y_pred.append(y_pred.data.cpu())\n",
    "                loss_T += loss.item()\n",
    "            Y = torch.cat(Y)\n",
    "            Y_pred = torch.cat(Y_pred)\n",
    "#             accuracy = torch.sum(Y==Y_pred).item() / len(Y)\n",
    "            accuracy = acc(Y.numpy(), Y_pred.numpy())\n",
    "            print(f'Loss = {loss_T/len(trainloader)}  Accuracy={accuracy}')\n",
    "    \n",
    "    def validate_classifier(self, trainloader):\n",
    "        if(self.use_cuda):\n",
    "            self.cuda()\n",
    "        self.eval()\n",
    "        optimizer = optim.Adam(filter(lambda p: p.requires_grad, self.parameters()), lr=self.lr)\n",
    "        loss_T = 0\n",
    "        Y = []\n",
    "        Y_pred = []\n",
    "        for inputs, labels in trainloader:\n",
    "            if(self.use_cuda):\n",
    "                inputs = inputs.cuda()\n",
    "                labels = labels.cuda()\n",
    "\n",
    "            inputs = Variable(inputs)\n",
    "\n",
    "            z, q, _ = self.forward(inputs)\n",
    "            logits = self.out_linear(z)\n",
    "            loss = self.cross_entropy(logits, labels)\n",
    "\n",
    "            Y.append(labels.cpu())\n",
    "            y_pred = torch.argmax(logits, 1)\n",
    "            Y_pred.append(y_pred.data.cpu())\n",
    "            loss_T += loss.item()\n",
    "        Y = torch.cat(Y)\n",
    "        Y_pred = torch.cat(Y_pred)\n",
    "#         accuracy = torch.sum(Y==Y_pred).item() / len(Y)\n",
    "        accuracy = acc(Y.numpy(), Y_pred.numpy())\n",
    "        print(f'Loss = {loss_T/len(trainloader)}  Accuracy={accuracy}')\n",
    "        \n",
    "        \n",
    "    def fit(self, dataloader, validloader, update_interval=1, tol=1e-3):\n",
    "        '''X: tensor data'''\n",
    "        X = None\n",
    "        y = None\n",
    "        for featuresT,labelsT in dataloader:\n",
    "            X = featuresT\n",
    "            y = labelsT\n",
    "        batch_size = self.batch_size\n",
    "        num_epochs = self.n_epochs\n",
    "        lr = self.lr\n",
    "        if(self.use_cuda):\n",
    "            self.cuda()\n",
    "        print(\"=====Training IDEC=======\")\n",
    "        optimizer = optim.Adam(filter(lambda p: p.requires_grad, self.parameters()), lr=lr)\n",
    "        # optimizer = optim.SGD(filter(lambda p: p.requires_grad, self.parameters()), lr=lr, momentum=0.9)\n",
    "\n",
    "        self.train()\n",
    "        self.apply(set_bn_eval)\n",
    "        print(\"Initializing cluster centers with kmeans.\")\n",
    "        kmeans = KMeans(self.n_clusters, n_init=20)\n",
    "        data = self.encodeBatch(X)\n",
    "        y_pred = kmeans.fit_predict(data.data.cpu().numpy())\n",
    "        y_pred_last = y_pred\n",
    "        self.mu.data.copy_(torch.Tensor(kmeans.cluster_centers_))\n",
    "        if y is not None:\n",
    "            y = y.cpu().numpy()\n",
    "            print(\"Kmeans acc: %.5f, nmi: %.5f\" % (acc(y, y_pred), normalized_mutual_info_score(y, y_pred)))\n",
    "\n",
    "        num = X.shape[0]\n",
    "        num_batch = int(math.ceil(1.0*X.shape[0]/batch_size))\n",
    "        for epoch in range(num_epochs):\n",
    "            self.train()\n",
    "            self.apply(set_bn_eval)\n",
    "            for featuresT,labelsT in dataloader:\n",
    "                X = featuresT\n",
    "                y = labelsT\n",
    "            if epoch%update_interval == 0:\n",
    "                # update the targe distribution p\n",
    "                latent = self.encodeBatch(X)\n",
    "                q = self.soft_assign(latent)\n",
    "                p = self.target_distribution(q).data\n",
    "\n",
    "                # evalute the clustering performance\n",
    "                y_pred = torch.argmax(q, dim=1).data.cpu().numpy()\n",
    "                if y is not None:\n",
    "                    y = y.cpu().numpy()\n",
    "                    \n",
    "                    print(\"acc: %.5f, nmi: %.5f\" % (acc(y, y_pred), normalized_mutual_info_score(y, y_pred)))\n",
    "\n",
    "                # check stop criterion\n",
    "                delta_label = np.sum(y_pred != y_pred_last).astype(np.float32) / num\n",
    "                y_pred_last = y_pred\n",
    "                if epoch>0 and delta_label < tol:\n",
    "                    print('delta_label ', delta_label, '< tol ', tol)\n",
    "                    print(\"Reach tolerance threshold. Stopping training.\")\n",
    "                    break\n",
    "\n",
    "            # train 1 epoch\n",
    "            self.apply(set_bn_train)\n",
    "            train_loss = 0.0\n",
    "            for batch_idx in range(num_batch):\n",
    "                xbatch = X[batch_idx*batch_size : min((batch_idx+1)*batch_size, num)]\n",
    "                pbatch = p[batch_idx*batch_size : min((batch_idx+1)*batch_size, num)]\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                inputs = Variable(xbatch)\n",
    "                target = Variable(pbatch)\n",
    "                if(self.use_cuda):\n",
    "                    inputs = inputs.cuda()\n",
    "                    target = target.cuda()\n",
    "                z, qbatch, xrecon = self.forward(inputs)\n",
    "                loss = self.loss_function(inputs, xrecon, target, qbatch)\n",
    "                train_loss += loss.item()*len(inputs)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            print(\"#Epoch %3d: Loss: %.4f\" % (\n",
    "                epoch+1, train_loss / num))\n",
    "            \n",
    "            if(epoch%5 == 0):\n",
    "                self.eval_clustering(validloader)\n",
    "            \n",
    "    \n",
    "#     def train_semisupervised(self, dataloader, dataloader_l, validloader, triplet_selector=None, update_interval=1, tol=1e-3):\n",
    "#         self.train_triplet(dataloader_l, validloader, triplet_selector)\n",
    "#         self.fit(dataloader, validloader)\n",
    "        \n",
    "#     def train_supervised(self, dataloader)\n",
    "    \n",
    "    def train_semisupervised(self, dataloader, dataloader_l, validloader, update_interval=1, tol=1e-3):\n",
    "        '''X: tensor data'''\n",
    "        X = None\n",
    "        y = None\n",
    "        for featuresT,labelsT in dataloader:\n",
    "            X = featuresT\n",
    "            y = labelsT\n",
    "        batch_size = self.batch_size\n",
    "        num_epochs = self.n_epochs\n",
    "        lr = self.lr\n",
    "        if(self.use_cuda):\n",
    "            self.cuda()\n",
    "        print(\"=====Training IDEC=======\")\n",
    "        optimizer = optim.Adam(filter(lambda p: p.requires_grad, self.parameters()), lr=lr)\n",
    "        # optimizer = optim.SGD(filter(lambda p: p.requires_grad, self.parameters()), lr=lr, momentum=0.9)\n",
    "        \n",
    "        print(\"Initializing cluster centers with kmeans.\")\n",
    "        kmeans = KMeans(self.n_clusters, n_init=20)\n",
    "        data = self.encodeBatch(X)\n",
    "        y_pred = kmeans.fit_predict(data.data.cpu().numpy())\n",
    "        y_pred_last = y_pred\n",
    "        self.mu.data.copy_(torch.Tensor(kmeans.cluster_centers_))\n",
    "        if y is not None:\n",
    "            y = y.cpu().numpy()\n",
    "            print(\"Kmeans acc: %.5f, nmi: %.5f\" % (acc(y, y_pred), normalized_mutual_info_score(y, y_pred)))\n",
    " \n",
    "        num = X.shape[0]\n",
    "        num_batch = int(math.ceil(1.0*X.shape[0]/batch_size))\n",
    "        dataiter_l = iter(dataloader_l)\n",
    "        for epoch in range(num_epochs):\n",
    "            self.train()\n",
    "            self.visualise_embeddings(validloader, f'{epoch}')\n",
    "            for featuresT,labelsT in dataloader:\n",
    "                X = featuresT\n",
    "                y = labelsT\n",
    "            if epoch%update_interval == 0:\n",
    "                # update the targe distribution p\n",
    "                latent = self.encodeBatch(X)\n",
    "                q = self.soft_assign(latent)\n",
    "                if(epoch>30):\n",
    "                    self.save_images(X.data.cpu().numpy(), q.data.cpu().numpy())\n",
    "                    assert 1==2\n",
    "                p = self.target_distribution(q).data\n",
    "\n",
    "                # evalute the clustering performance\n",
    "                y_pred = torch.argmax(q, dim=1).data.cpu().numpy()\n",
    "                if y is not None:\n",
    "                    y = y.cpu().numpy()\n",
    "                    \n",
    "                    print(\"acc: %.5f, nmi: %.5f\" % (acc(y, y_pred), normalized_mutual_info_score(y, y_pred)))\n",
    "\n",
    "                # check stop criterion\n",
    "                delta_label = np.sum(y_pred != y_pred_last).astype(np.float32) / num\n",
    "                y_pred_last = y_pred\n",
    "                if epoch>0 and delta_label < tol:\n",
    "                    print('delta_label ', delta_label, '< tol ', tol)\n",
    "                    print(\"Reach tolerance threshold. Stopping training.\")\n",
    "                    break\n",
    "\n",
    "            # train 1 epoch\n",
    "            train_loss = 0.0\n",
    "            for batch_idx in range(num_batch):\n",
    "                \n",
    "                try:\n",
    "                    inputs_l, labels_l = next(dataiter_l)\n",
    "                except StopIteration:\n",
    "                    dataiter_l = iter(dataloader_l)\n",
    "                    inputs_l, labels_l = next(dataiter_l)\n",
    "                \n",
    "                if(self.use_cuda):\n",
    "                    inputs_l = inputs_l.cuda()\n",
    "                    labels_l = labels_l.cuda()\n",
    "                    \n",
    "                inputs_l = Variable(inputs_l)\n",
    "                \n",
    "                z_l, q_l, _ = self.forward(inputs_l)\n",
    "                \n",
    "                xbatch = X[batch_idx*batch_size : min((batch_idx+1)*batch_size, num)]\n",
    "                pbatch = p[batch_idx*batch_size : min((batch_idx+1)*batch_size, num)]\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                inputs = Variable(xbatch)\n",
    "                target = Variable(pbatch)\n",
    "                if(self.use_cuda):\n",
    "                    inputs = inputs.cuda()\n",
    "                    target = target.cuda()\n",
    "                z, qbatch, xrecon = self.forward(inputs)\n",
    "                loss_u = self.loss_function(inputs, xrecon, target, qbatch)\n",
    "#                 logits = self.out_linear(z_l)\n",
    "                loss_l = self.cross_entropy(q_l, labels_l)\n",
    "                loss = loss_l #+ loss_u\n",
    "                train_loss += loss.item()*len(inputs)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            print(\"#Epoch %3d: Loss: %.4f\" % (\n",
    "                epoch+1, train_loss / num))\n",
    "            \n",
    "            if(epoch%10 == 0):\n",
    "                self.eval()\n",
    "                z_agg = []\n",
    "                y = []\n",
    "                for batch_idx, (inputs, labels) in enumerate(validloader):\n",
    "                    inputs = Variable(inputs)\n",
    "                    if(self.use_cuda):\n",
    "                        inputs = inputs.cuda()\n",
    "                    z, _, mux = self.forward(inputs)\n",
    "                    z_agg.append(z)\n",
    "                    y.extend(labels.data.cpu().numpy())\n",
    "                \n",
    "                y = np.array(y)\n",
    "                latent = torch.cat(z_agg, dim=0)\n",
    "                y_pred = kmeans.fit_predict(latent.data.cpu().numpy())\n",
    "                if y is not None:\n",
    "                    print(\"***Kmeans Validation acc: %.5f, nmi: %.5f\" % (acc(y, y_pred), normalized_mutual_info_score(y, y_pred)))\n",
    "\n",
    "                \n",
    "                q = self.soft_assign(latent)\n",
    "                # evalute the clustering performance\n",
    "                y_pred = torch.argmax(q, dim=1).data.cpu().numpy()\n",
    "                if y is not None:\n",
    "                    print(\"***Validation acc: %.5f, nmi: %.5f\" % (acc(y, y_pred), normalized_mutual_info_score(y, y_pred)))\n",
    "        \n",
    "        \n",
    "    def save_images(self, X, q):\n",
    "        y_pred = np.argmax(q, axis=1)\n",
    "        q = np.amax(q, axis=1)\n",
    "        for i in range(10):\n",
    "            X_subset = X[y_pred == i]\n",
    "            q_subset_ind = np.argsort(q[y_pred == i])\n",
    "            X_subset = X_subset[q_subset_ind]\n",
    "            X_subset = X_subset[-10:]\n",
    "#             print(X_subset.shape)\n",
    "            for k, data in enumerate(X_subset):\n",
    "                data = data[0]\n",
    "                rescaled = (255.0 / data.max() * (data - data.min())).astype(np.uint8)\n",
    "                im = Image.fromarray(rescaled)\n",
    "                im.save(f'{self.results_dir}/{i}_{k}.png')\n",
    "        \n",
    "    def visualise_embeddings(self, trainloader, imname=''):\n",
    "        if(self.use_cuda):\n",
    "            self.cuda()\n",
    "        self.eval()\n",
    "        z_agg = []\n",
    "        labels_agg = []\n",
    "        inputs_agg = []\n",
    "        for batch_idx, (inputs, labels) in enumerate(trainloader):\n",
    "            if(self.use_cuda):\n",
    "                inputs = inputs.cuda()\n",
    "            inputs = Variable(inputs)\n",
    "            inputs_agg.append(inputs)\n",
    "            z, _, _ = self.forward(inputs)\n",
    "            z_agg.append(z.data.cpu().numpy())\n",
    "            labels_agg.extend(labels.data.cpu().numpy())\n",
    "            \n",
    "        labels_agg = np.array(labels_agg)\n",
    "        z_agg = np.concatenate(z_agg, axis=0)\n",
    "#         z_agg = z_agg.data.cpu().numpy()\n",
    "        z_compressed = TSNE(n_components=2).fit_transform(z_agg)\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.scatter(z_compressed[:, 0], z_compressed[:, 1], c=labels_agg, cmap='brg')\n",
    "        plt.colorbar()\n",
    "        plt.savefig(f'{self.results_dir}/{imname}_emb.png')\n",
    "        plt.close('all')  \n",
    "            \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MNSIT_transform = transforms.Compose([transforms.ToTensor(),\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Pad(2),\n",
    "    transforms.ToTensor(),])\n",
    "\n",
    "batch_size = 120\n",
    "traindataset = datasets.FashionMNIST('../Data/FashionMNIST', \n",
    "                           download=True, \n",
    "                           train=True, \n",
    "                           transform=MNSIT_transform)\n",
    "testdataset = datasets.FashionMNIST('../Data/FashionMNIST', \n",
    "                           download=True, \n",
    "                           train=False, \n",
    "                           transform=MNSIT_transform)\n",
    "fulldata = torch.utils.data.ConcatDataset([traindataset, testdataset])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(traindataset,\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(testdataset,\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=True)\n",
    "\n",
    "valid_loader_full = torch.utils.data.DataLoader(testdataset,\n",
    "                                        batch_size=batch_size,\n",
    "                                        shuffle=False)\n",
    "# traindataset = CustomDataset1()\n",
    "\n",
    "# full_loader = torch.utils.data.DataLoader(fulldata,\n",
    "#                                          batch_size=batch_size,\n",
    "#                                          shuffle = True)\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(traindataset,\n",
    "                                         batch_size = len(traindataset),\n",
    "                                         shuffle = True)\n",
    "data_loader_test = torch.utils.data.DataLoader(testdataset,\n",
    "                                         batch_size = len(testdataset),\n",
    "                                         shuffle = True)\n",
    "\n",
    "for featuresT,labelsT in data_loader:\n",
    "    features = featuresT.numpy()\n",
    "    labels = labelsT.numpy()\n",
    "\n",
    "_, X_supervised, _, y_supervised = train_test_split(features, labels,\n",
    "                                            stratify=labels, \n",
    "                                            test_size=0.166,\n",
    "                                            random_state=42)\n",
    "\n",
    "\n",
    "trainset_su = CustomDataset3(X_supervised,y_supervised)\n",
    "train_loader_su = torch.utils.data.DataLoader(trainset_su,\n",
    "                                            batch_size=len(trainset_su),\n",
    "                                            shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_sampler = BalancedBatchSampler(trainset_su, n_classes=10, n_samples=5)\n",
    "# test_batch_sampler = BalancedBatchSampler(testdataset, n_classes=10, n_samples=25)\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if torch.cuda.is_available() else {}\n",
    "batch_train_loader_su = torch.utils.data.DataLoader(trainset_su, batch_sampler=train_batch_sampler, **kwargs)\n",
    "# online_test_loader = torch.utils.data.DataLoader(testdataset, batch_sampler=test_batch_sampler, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train simple cross entropy classifier\n",
    "dectvae = DecTvae(n_epochs = 30)\n",
    "dectvae.train_classifier(batch_train_loader_su)\n",
    "dectvae.validate_classifier(valid_loader_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====Training IDEC=======\n",
      "Initializing cluster centers with kmeans.\n",
      "Kmeans acc: 0.51562, nmi: 0.49439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rchanumo/anaconda3/envs/RAship/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:844: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n",
      "/home/rchanumo/anaconda3/envs/RAship/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:844: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.51753, nmi: 0.49482\n",
      "#Epoch   1: Loss: 1.7560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rchanumo/anaconda3/envs/RAship/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:844: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Kmeans Validation acc: 0.78660, nmi: 0.73240\n",
      "***Validation acc: 0.79730, nmi: 0.73796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rchanumo/anaconda3/envs/RAship/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:844: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.80490, nmi: 0.74670\n",
      "#Epoch   2: Loss: 1.6552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rchanumo/anaconda3/envs/RAship/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:844: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.81953, nmi: 0.76973\n",
      "#Epoch   3: Loss: 1.6355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rchanumo/anaconda3/envs/RAship/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:844: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.81340, nmi: 0.76435\n",
      "#Epoch   4: Loss: 1.6255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rchanumo/anaconda3/envs/RAship/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:844: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.82155, nmi: 0.77067\n",
      "#Epoch   5: Loss: 1.6212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rchanumo/anaconda3/envs/RAship/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:844: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.82992, nmi: 0.77843\n",
      "#Epoch   6: Loss: 1.6126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rchanumo/anaconda3/envs/RAship/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:844: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.83183, nmi: 0.78320\n",
      "#Epoch   7: Loss: 1.6040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rchanumo/anaconda3/envs/RAship/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:844: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.81960, nmi: 0.76215\n",
      "#Epoch   8: Loss: 1.5998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rchanumo/anaconda3/envs/RAship/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:844: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.83863, nmi: 0.77560\n",
      "#Epoch   9: Loss: 1.5794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rchanumo/anaconda3/envs/RAship/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:844: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.86618, nmi: 0.78179\n",
      "#Epoch  10: Loss: 1.5619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rchanumo/anaconda3/envs/RAship/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:844: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.87258, nmi: 0.79009\n",
      "#Epoch  11: Loss: 1.5570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rchanumo/anaconda3/envs/RAship/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:844: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Kmeans Validation acc: 0.85890, nmi: 0.77555\n",
      "***Validation acc: 0.85830, nmi: 0.77445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rchanumo/anaconda3/envs/RAship/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:844: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.87628, nmi: 0.79783\n",
      "#Epoch  12: Loss: 1.5514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rchanumo/anaconda3/envs/RAship/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:844: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.87737, nmi: 0.79476\n",
      "#Epoch  13: Loss: 1.5454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rchanumo/anaconda3/envs/RAship/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:844: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.87978, nmi: 0.79846\n",
      "#Epoch  14: Loss: 1.5437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rchanumo/anaconda3/envs/RAship/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:844: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.85807, nmi: 0.78406\n",
      "#Epoch  15: Loss: 1.5402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rchanumo/anaconda3/envs/RAship/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:844: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.88222, nmi: 0.80026\n",
      "#Epoch  16: Loss: 1.5375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rchanumo/anaconda3/envs/RAship/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:844: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.88155, nmi: 0.80041\n",
      "#Epoch  17: Loss: 1.5360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rchanumo/anaconda3/envs/RAship/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:844: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.87397, nmi: 0.79521\n",
      "#Epoch  18: Loss: 1.5327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rchanumo/anaconda3/envs/RAship/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:844: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.87847, nmi: 0.79686\n",
      "#Epoch  19: Loss: 1.5317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rchanumo/anaconda3/envs/RAship/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:844: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.87768, nmi: 0.79513\n",
      "#Epoch  20: Loss: 1.5321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rchanumo/anaconda3/envs/RAship/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:844: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.87937, nmi: 0.79493\n",
      "#Epoch  21: Loss: 1.5271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rchanumo/anaconda3/envs/RAship/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:844: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Kmeans Validation acc: 0.85960, nmi: 0.77222\n",
      "***Validation acc: 0.85950, nmi: 0.77237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rchanumo/anaconda3/envs/RAship/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:844: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.88258, nmi: 0.80032\n",
      "#Epoch  22: Loss: 1.5274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rchanumo/anaconda3/envs/RAship/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:844: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.88078, nmi: 0.80060\n",
      "#Epoch  23: Loss: 1.5214\n"
     ]
    }
   ],
   "source": [
    "dectvae = DecTvae(n_epochs = 40, dataset='FashionMNIST_NN')\n",
    "# dectvae.pretrain(train_loader, valid_loader_full)\n",
    "# dectvae.save_model('pretrain_w_BN.pt')\n",
    "dectvae.load_model('pretrain_w_BN.pt')\n",
    "# dectvae.fit(data_loader, valid_loader_full)\n",
    "# dectvae.save_model('model_w_BN.pt')\n",
    "dectvae.train_semisupervised(data_loader, batch_train_loader_su, valid_loader_full)\n",
    "# dectvae.save_model('baseline_100.pt')\n",
    "# dectvae.load_model('baseline_100.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dectvae.visualise_embeddings(test_loader, '1000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([1,2,3])\n",
    "b = torch.tensor([1,2,6])\n",
    "torch.cat([a,b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "png.from_array([[255, 0, 0, 255],\n",
    "                [0, 255, 255, 0]], 'L').save(\"small_smiley.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
